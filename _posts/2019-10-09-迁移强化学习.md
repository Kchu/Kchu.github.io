---
layout: post
title:  "迁移强化学习综述"
date:   2019-10-9 10:56:56 +0800
<!-- categories: RL -->
tags: 强化学习
---

## 1. 迁移学习目标
在强化学习(RL)问题中，学习主体以最大化奖励信号为目标，采取连续的动作，这可能是延时的。例如，一个代理可以通过被告知它是赢还是输来学习玩游戏，但是从来不会在任何给定的时间点被给予“正确”的操作。随着能够处理日益复杂的问题的学习方法的发展，RL框架得到了普及。然而，当RL代理开始学习tabula rasa时，掌握困难的任务往往很慢或不可行，因此，当前大量的RL研究都集中在利用具有不同数量人类提供的知识的领域专长来提高学习速度。

常见的方法有：将任务分解为子任务的层次结构；用更高层次的、时序抽象的行动(如选项，options)而不是简单的一步动作来学习；有效地对状态空间进行抽象(例如通过函数逼近)，从而使代理能够更有效地概括其经验。

迁移学习(TL)背后的观点是，泛化不仅可能发生在任务内部，也可能发生在任务之间。

本文转而关注RL任务之间的传输(参见图1)，以提供一个新的、不断发展的研究领域的概述
![4cdbf0df613a404cfc8a5a7f08d91b6f.png](:/eaa1fa49546e4216841c77b4a89e111c)
RL中的迁移学习是当前需要解决的一个重要课题，原因有三。
首先，RL技术近年来取得了显著的成功在困难的任务，其他机器学习技术是不能或不具备达到的条件。
其次，经典的机器学习技术，如规则归纳和分类，已经足够成熟，现在可以很容易地利用它们来辅助迁移学习。
第三，有希望的初步结果表明，这种迁移方法不仅是可能的，而且可以非常有效地加快学习。

### 1.1 论文概览
本研究的目的是向读者介绍RL领域的迁移学习问题，组织和讨论当前的迁移方法，并列举RL迁移中的重要开放问题。在转移过程中，来自一个或多个源任务的知识被用来比不使用转移时更快地学习一个或多个目标任务。所调查的文献主要是根据源任务和目标任务的不同而采用的分组方法。我们根据五个不同的维度进一步区分方法(参见2.2节)。一些区别转移方法的问题包括：
* 转移的目的是什么？衡量成功的标准是什么？第2节讨论了常用的度量标准，以及迁移学习可以改进学习的不同设置。
* 对于任务之间的相似性有什么假设(如果有的话)？第3.2.1节列举了常见的差异，如对代理动作空间的更改、允许代理具有不同的目标或允许代理具有不同的操作集。
* 转移方法如何识别哪些信息可以/应该转移？第3.2.2节列举了各种可能性，从假设所有以前看到的任务都是直接有用的，到自主学习哪些源任务对当前目标任务的学习是有用的。
* 任务之间传递什么信息？第3.2.3节讨论了从非常低等级的信息(如直接控制知识)到高等级信息(如关于特定域如何工作的规则)的各种可能性。

下一节将讨论如何在RL中最好地评估迁移。在许多不同的情况下，转移可能是有用的，这些不同的情况可能包含不同的度量标准。这个讨论将使读者更好地理解如何使用迁移。第3.1节将简要讨论强化学习和本文中使用的符号。第3.2节列举了转移方法的不同之处，为本次调查的结构提供了框架。3.3节和3.4节提供了TL方法的其他高级分类，3.5节讨论了本调查中没有明确讨论的相关学习范式。
本文的其余部分(第4-8节)主要讨论现在的TL方法，这些方法由设计者的目标和提出来的方法安排。最后，第9节讨论了当前迁移学习中的未决问题并得出结论。

## 2. 评价迁移学习方法
迁移技术具有不同程度的自主性，并做出许多不同的假设。为了完全自主，RL传输代理必须执行以下所有步骤：
1.给定目标任务，选择要从中迁移的合适的源任务或任务集。
2.了解（多个）源任务和目标任务之间的关系。
3.有效地将知识从（多个）源任务迁移到目标任务。

虽然这些步骤所使用的机制必然是相互依赖的，但是TL的研究主要集中在每一个独立的方面，目前还没有一种TL方法能够完全实现这三个目标。

TL研究中的一个关键挑战是定义评价指标，因为有许多可能的度量选项，而且算法可能集中于上述三个步骤中的任何一个。本节主要讨论如何最好地评估TL算法，以便读者更好地理解迁移的不同目标以及迁移可能带来的好处。例如，如何对待源任务中的学习并不总是很清楚：是将其归咎于TL算法，还是将其视为“沉没成本”。一方面，转移的一个可能的目标是减少学习一项复杂任务所需的总时间。在这个场景中，一个明确包含学习源任务或任务所需时间的**总时间场景**是最合适的。另一方面，第二个合理的转移目标是在一个新的任务中有效地重用过去的知识。在这种情况下，**目标任务时间场景**是合理的，它只考虑在目标任务中花费的学习时间。

当一个代理由人显式地指导时，总时间场景可能更合适。假设用户希望代理学习执行任务，但是认识到与直接处理困难任务相比，代理可能能够更快地学习一系列任务。人可以为agent构造一系列的任务，向agent提示这些任务之间的关系。因此代理的TL方法很容易完成上面的步骤1和2，但是它必须有效转移知识之间的任务(步骤3)。在此设置成功转移，代理必须学习整个任务序列要比它花了相同的时间直接学习最终目标任务要快(见图2中的总时间场景)。

![a34eb745051ce0b8f87e3d207389d5d1.png](:/eae6a85f29b34b45a09fa48acdfeb426)

目标任务时间场景更适合一个完全自主的学习者。完全自主的代理必须能够自己执行步骤1-3。但是，此场景的度量不需要考虑学习源任务的成本。目标任务时间场景强调代理的使用知识从一个或多个先前学习的源任务在未被起诉的时间内学习的能力(见图2目标任务时间场景)。在这个调查我们将看到，大部分现有的迁移算法假设一个人类引导的场景，但忽视在源任务中培训的时间。在讨论单个TL方法时，我们将特别注意那些占总培训时间的方法，而不将用于学习源任务的时间视为沉没成本。

列举一下度量指标：
* 1. Jumpstart（启动）：通过从源任务迁移代理，可以改进目标任务中agent的初始性能。
* 2. Asymptotic Performance（渐近性能）：通过迁移可以提高目标任务中agent的最终学习性能。
* 3. Total Reward（总报酬）：相对于不使用迁移的学习，agent如果使用迁移，可能会改善的所累积的总报酬(即学习曲线下的面积)。
* 4. Transfer Ratio（迁移率）：迁移学习者累计的总报酬与非迁移学习者累计的总报酬之比。
* 5. Time to Threshold（阈值时间）：通过知识迁移，可以减少agent达到预定性能水平所需的学习时间。

度量标准1-4最适合于完全自主的场景，因为它们不向代理收取用于学习任何源任务的时间费用。要度量总时间，度量必须考虑学习一个或多个源任务所花费的时间，这在使用度量5时是很自然的。文献中已经提出了其他指标，但是我们选择关注这五个指标，因为它们足以描述本文中调查的方法。

对于本文，我们可以将学习时间看作是样本复杂性(*Sample complexity*)的替代物。RL中的样本复杂度(或数据复杂度)是指算法学习所需的数据量它与学习时间密切相关，因为RL代理仅通过与环境的重复交互来收集数据。

### 2.1 实证转移比较
前一节列举了五种可能的TL度量标准，虽然其他度量标准也是可能的，但它们代表了最常用的方法。然而，每个度量都有缺点，没有一个足以充分描述任何转移方法的优点。与其试图为不同的方法创建一个完全有序的排序(这实际上是不可能的)，我们建议使用**具有多个度量的多维评估**是最有用的。具体来说，一些方法可能在一组度量标准上相对于其他方法“成功”，但是在另一组度量标准上“失败”。随着这个领域更好地理解为什么不同的方法在不同的度量标准上获得不同程度的成功，将TL方法适当地映射到TL问题应该会变得更容易。虽然机器学习社区已经定义了标准的度量标准(例如分类的精度与召回率曲线和回归的均方误差)，但是RL没有这样的标准。从经验上比较两种RL算法是当前社区内争论的一个主题，尽管有一些标准化比较的过程。理论比较也不明确，因为样本的收敛性、渐近性能和计算复杂度都是评价RL算法的有效轴。

**如何更好的评价或者度量迁移学习方法，就是需要综合这些方法。**
下图是对各个评价指标的图示，帮助更好地理解这几个标准。
![f79572ef58bf6857de787d2a53168674.png](:/2db009c17bba4f39bffa1ae36d42678e)
### 如何理解这几个度量标准？
1. 第一个提出的迁移度量考虑了agent在目标任务中的初始性能，并回答了这样一个问题，可以使用迁移使初始性能相对于初始(随机)策略的性能提高吗?虽然这样的初始跳跃很吸引人，但是这样的度量不能捕获目标任务中的学习行为，而是只关注学习发生之前的性能。
2. 第二种度量方法是渐近性能，它比较了有迁移和无迁移时学习者在目标任务中的最终性能。然而，可能很难判断学习者何时真正收敛(特别是在具有无限状态空间的任务中)，或者收敛可能需要非常长的时间。在许多情况下，需要学习的样本数量是最关键的，而不是一个拥有无数样本的学习者的表现。此外，不同的学习算法有可能收敛到相同的渐近性能，但需要非常不同的样本数才能达到相同的性能。
3. 第三个可能的措施是在培训期间累积的总奖励。提高初始性能和更快的学习速度将帮助代理积累更多的在线奖励。RL方法常常不能保证收敛于函数逼近，即使收敛，学习者也可能收敛到不同的次优性能水平。如果向agent提供足够的样本(或者，同样地，向学习者提供足够的训练时间)，相对于学习速度很慢但最终达到稍高的性能水平的学习方法，相对于学习速度较快的学习方法获得的总回报更少。此度量最适合具有定义良好持续时间的任务。
4. 第四个衡量迁移效率的指标是两条学习曲线所定义的面积之比。考虑目标任务中的两条学习曲线，一条使用迁移，另一条不使用。假设迁移学习者获得更多的奖励，那么迁移学习曲线下的面积将大于非迁移学习曲线下的面积。这个比率提供了一个度量标准，它可以量化TL的改进。如果达到了相同的最终性能，或者任务有一个预先确定的时间，那么这个度量标准是最合适的。否则，该比率将直接取决于代理在目标任务中的执行时间。
![5758f8e7f1e6ca496e2bf0d0b6dd630e.png](:/f58f2b5f0ba349d680c013b74f2851d4)
虽然作为任务间比较的候选指标，这样的度量可能很有吸引力，但是我们注意到传输比率并不是规模不变的。例如，如果迁移曲线下的面积为1000单位，非迁移曲线下的面积为500，那么迁移比为1.0。如果所有的奖励都乘以一个常数，这个比例就不会改变。但是如果增加一个偏移量(例如，每个代理在每个episode结束时都会得到一个额外的+1，不管最后的状态如何)，这个比例就会改变。因此，具有迁移率的TL算法的评价与被测目标任务的奖励结构密切相关。最后，我们注意到，虽然本文调查的所有论文都没有使用这种度量标准，但我们希望将来能更经常地使用它。
5. 最后一个度量，到阈值的时间，必须指定(可能是任意的)agent必须达到的性能。虽然已经有一些关于如何适当选择这些阈值的建议，但是TL方法的相对优势显然取决于所选择的确切阈值，这必然依赖于领域和学习方法。虽然选择一个阈值范围进行比较可能会产生更具代表性的度量，但这导致必须生成一个时间与阈值曲线，而不是生成一个评估迁移算法有效性的单一实数。

与上述方法相结合的另高层次的分析是计算一个比率，比较TL算法与人类学习者的性能。例如，一组人类受试者可以学习给定的目标任务，而不需要在源任务进行训练。通过对它们的性能进行平均，可以计算出不同的人员迁移指标，并与TL算法进行比较。然而，有许多方法可以操作这样的元度量(meta-metric)。例如，如果选择了一个人类比较擅长的目标任务，那么迁移将给他们带来很少的好处。如果同样的目标任务对于机器学习算法来说是困难的，那么相对于人工迁移来说，TL算法是相当有效的，即使agent的绝对性能非常差。

所有讨论的度量的一个主要缺点是**没有一个适合域间比较**。这项调查中的绝大多数论文都比较了有迁移和没有迁移的学习——他们的作者通常不会尝试直接比较不同的迁移方法。开发适用于多个问题领域的公平指标将有助于更好地比较方法。这样的域间指标可能在实践中不可行，在这种情况下，标准化一组测试域将方便在比较不同的TL方法(如9节中进一步讨论)。在缺乏一系列域间的指标或标准基准套件的领域，我们限制我们比较不同的TL方法的适用性假设和算法的差异。当讨论不同的方法时，我们可能会对方法的相对性能发表评论，但是我们要提醒读者，这样的评论很大程度上是基于直觉，而不是经验数据。

### 2.2 比较维度
除了评价指标的不同，我们还将TL算法按照五个维度进行分类，并将其作为文献调查的主要组织框架：
1. Task difference assumptions(任务差异假设)：
TL方法对源和目标之间的差异做了哪些假设?源任务和目标任务之间可能存在差异的例子包括不同的系统动态，或在某些状态下的不同的可能动作集。这些假设定义了方法可以在源任务和目标任务之间迁移的类型。允许在不太相似的源任务和目标任务之间进行转换，可以在人工指导的场景中为人工设计人员提供更大的灵活性。在完全自主的场景中，更灵活的方法更有可能成功地将过去的知识应用到新的目标任务中。
2. Source task selection(源任务选择)：
在最简单的情况下，agent假定一个人已经执行了源任务选择(人工指导的场景)，并从一个或多个选择的任务进行了迁移。更复杂的方法允许agent选择源任务或一组源任务。此外，这种选择机制还可以防止负迁移，负迁移会损害学习者的表现。选择机制越健全，迁移就越有可能带来好处。虽然这个问题没有明确的答案，但是成功的技术可能必须考虑到特定的目标任务特征。例如，Carroll和Seppi(2005)激发了对一般任务相似度度量的需求，以支持健壮的迁移，提出了三个不同的度量，然后继续证明没有一个总是“最好的”，就像在学习算法中从来没有“最好的”归纳偏见一样。
3. Task Mapping(任务映射)：
许多方法需要映射才能有效地迁移：除了知道源任务和目标任务是相关的外，它们还需要知道它们是如何关联的。任务间映射(稍后将在3.4节中详细讨论)是定义两个任务之间关系的一种方法。如果一个人在循环中，该方法可以假设提供了这样的任务映射；如果agent期望自动传输，则必须学习此类映射。不同的方法使用不同的技术来实现迁移，包括在线(在学习目标任务时)和离线(在学习源任务后但在学习目标任务之前)。这种学习方法试图最小化所需的样本数量和/或学习方法的计算复杂度，同时仍然学习映射以实现有效的转换
4. Transferred Knowledge(迁移的知识)：
在源任务和目标任务之间迁移的是什么类型的信息?这些信息可以是关于特定任务的非常低层次的信息(例如，当在特定位置执行某个操作时的预期结果)到试图指导学习的一般启发式。不同类型的知识可能更好地传递，也可能更差，这取决于任务相似性。例如，低级信息可能跨密切相关的任务传递，而高级概念可能跨不太相似的任务对传递。将知识从一个任务迁移到另一个任务的机制与正在迁移的内容、如何定义任务映射(3)以及对这两个任务做了哪些假设(1)密切相关。
5. Allow Learners(允许学习者)：
TL方法对使用什么RL算法有限制吗，比如只应用于时间差分方法?不同的学习算法有不同的偏差。理想情况下，实验人员或agent将根据任务的特性选择要使用的RL算法，而不根据TL算法。一些TL方法要求使用相同的方法学习源任务和目标任务，另一些方法允许在两个任务中都使用一组方法，但是最灵活的方法将代理的学习算法解耦到两个任务中。

另一个TL框架可以在Lazaric(2008)的相关工作部分中找到，这是最近的一篇关于RL任务中的TL的博士论文。Lazaric比较TL方法的好处的类型(启动、总报酬、和渐近性能)，允许的源和目标之间的差异(不同的目标，不同的过渡函数但相同的奖励函数，和不同的状态和动作空间)和迁移知识的类型(经验或知识结构)。我们的文章在考虑的方法数量、每种方法的描述深度以及使用不同的组织结构方面都更加详细。特别地，我们指定了哪一种方法改进了五个TL指标中的哪一个，我们注意到哪一种方法考虑了源任务培训时间，而不是将其视为沉没成本，我们根据上面的五个维度对方法进行了区分。

## 3. 强化学习迁移
在本节中，我们首先简要概述一些符号表示。然后，我们使用前面讨论的五个维度总结了本次调查中讨论的方法，并列举了这些维度的可能属性。最后，第3.5节讨论了目标类似于迁移的学习范式。

### 3.1 强化学习背景
RL问题通常是根据马尔可夫决策过程(MDPs)构建的(Puterman， 1994)。出于本文的目的，MDP和task可以互换使用。在MDP中，对于世界的当前状态$s∈S$存在一些可能的感知集，学习agent具有一个或多个初始初始状态$s_{initial}$。奖励函数$R： S→R$将环境的每个状态映射到一个数字，这个数字是到达该状态时获得的瞬时奖励。如果任务是episodic，agent始于一个开始状态和执行环境中的行为，直到它到达终端状态(一个或多个$s_{final}$，这可能被称为一个目标状态)，此时返回的代理是一个开始状态。episodic任务中的agent通常试图最大化每一步的平均报酬。在非episodic任务中，代理试图最大化总报酬，这可能会打折扣。通过使用折现系数$\gamma$，代理可以权衡即时回报比未来更大的奖励，让它最大化奖励的非无限和。

![33ddbebd3a4ee36a09edccd3f53014d0.png](:/c5d42b03f4d545ed86e4ee9422f5132b)

有许多可能的方法可以学习这样的策略：
* (1). Temporal Difference(时序差分)：比如Q-Learning等
* (2). Policy Search(策略搜索)：策略迭代，策略梯度，直接策略搜索。
* (3). Dynamic programming(动态规划)：其假设环境的完整模型已知，agent必须迭代地计算动作值函数的真实或近似值，并随着时间的推移进行改进。
* (4). Model-based or Model-learning methods(基于模型或者模型学习方法)：基于实例的方法(Instance based methods)，贝叶斯方法(Bayesian RL)。
* (5). Relational reinforcement learning(RRL，关系强化学习)
* (6). Batch learning methods(批学习)：最小二乘策略迭代(Least Squares Policy Iteration)，Fitted-Q Iteration，
* (7). Function approximation(函数近似)：随着状态空间的增长，如果状态空间是连续的，那么使用表就变得不切实际或不可能。
* (8). Options(选项) or Abstraction(抽象)：

### 3.2 迁移方法
#### 3.2.1 允许的任务差异
TL方法可以转移有着不同的过渡函数(用t表1中)，状态空间(s)，开始状态(si)，目标状态(sf)，状态变量(v)，奖励函数(r)，和/或动作集(a)的MDP。对于这些方法中的其中两个，agent的表示(agent-space，描述物理传感器和致动器)是相同的，而真正的状态变量和动作(problem-space，描述任务的状态变量和macro-actions)可以改变(表1中p，在第6节进一步讨论)。还有一个分支的工作重点关注迁移那些由可能在源和目标任务之间改变的一定数量的目标的任务，例如当在RRL中学习(表1#)。当总结允许任务的差异时，我们将专注于最重要的特征。例如，当源任务和目标任务允许有不同的状态变量和动作时，两个任务的状态空间不同，因为状态描述不同，转移函数和奖励函数也必须改变，但我们只指明“a”和“v”。

在mountain car示例任务中，这些差异可以表示为
变量|解释
-|:-:|-:
$t$|转移函数，使用更强大的汽车发动机或改变山的表面摩擦
$s$|状态空间，改变状态变量的范围
$s_{i}$|开始状态，每一个episode都要更换汽车的启动位置
$s_{f}$|目标状态，改变汽车的目标状态
$v$|agent-space，速度，仅通过其速度来描述agent的状态
$r$|奖励函数，不是每一步的奖励为- 1，而是与目标状态的距离有关
$a$|动作空间，禁用中性动作
$p$|problem-space，agent可以使用额外的状态变量来描述状态，比如前一个时间步上的速度，但是agent只直接测量当前的位置和速度
$#$|agent可能需要同时控制山上的两辆车

#### 3.2.2 源任务选择
有一些是人为选定(h)，有一些是(all)，有一些是建立一个可见任务的库(lib)，有一些任务是修改了的(mod)。

#### 3.2.3 迁移的知识
* Low-level： $<s，a，r，s′>$实例(I)；动作值函数(Q)；策略($\pi$)；全任务模型(model)；先验分布(pri)；
* High-level：在某些情况下使用什么动作(A，整个动作集的子集)；部分策略或选项($\pi_{p}$)；规则或建议(rule)；关于学习的重要特性(fea)；原始值函数(pvf：一种学习特征)；形成奖励(R)；子任务定义(sub)。

#### 3.2.4 任务匹配
本次调查中的大多数TL算法都假定不需要显式的任务映射，因为源任务和目标任务具有相同的状态变量和操作。除了具有相同的标签之外，状态变量和操作还需要在两个任务中具有相同的语义含义。
不使用映射(N/A)，人工使用映射(sup)，转移函数(T)，利用经验(exp)，动作匹配($M_{a}$)，状态被组合($sv_{g}$)。

#### 3.2.5 允许的学习者
时序差分(TD)，贝叶斯学习(B)，层次学习方法(H)，基于模型的学习(MB)，直接策略搜索(PS)，关系强化学习(RRL)，批学习(Batch)，基于案例推理(CBR)，线性规划(LP)。
### 3.3 多任务学习(MTL)
MTL和TL之间的主要区别是，多任务学习方法假定代理所经历的所有问题都来自相同的分布，而TL方法可能允许任意源任务和目标任务。
![0351e96fe12e642d5cabb85642bc8fe7.png](:/9db1d2065c9c4e9f969f629c18459f50)
![235c5b66c63601b79e34af017d0c3190.png](:/eb415d6fa891446a86cce061fa63c0fe)

在讨论监督多任务学习时，可以同时考虑多个任务的数据。在RL环境中，而不是同时学习多个问题(即在多个MDP中表现)，agent处理一系列的任务，这些任务比在TL设置中更紧密相关。

Sutton等人通过提出将单个大任务作为一系列子任务来处理是最合适的，从而激发了这种迁移方法。如果学习者能够跟踪它当前所处的子任务，那么它可能能够在不同的子任务之间传递知识，这些子任务可能都是相关的，因为它们都是相同的整体任务的一部分。

### 3.4 任务间映射

动作映射：Action mapping($χA$)
状态变量映射：State-variable mapping($χX$)
部分映射：Partial mapping，目标任务中任何新的动作都被忽略

注意：任务间映射不是函数，只有从源任务到目标任务的映射。

状态之间的映射，而不是状态变量之间的映射，有可能被用于迁移，尽管目前还没有研究过这个公式。另一个可能的扩展是链接映射，而不是使它们独立。例如，操作映射可能依赖于代理所处的状态，或者状态变量映射可能依赖于所选择的操作。尽管根据特定MDPs的需求，这些扩展可能是必要的，但是如果没有这些增强，当前的方法在各种任务中都能很好地工作。对于给定的一对任务，可以有许多方法来表示任务间映射。目前的TL工作大都假设人类向学习者提供了(正确的)映射。第8节讨论了尝试学习可以有效用于传输的映射的工作。

### 3.5 相关范例
#### 3.5.1 终生学习(Lifelong Learning)
终生学习就是一个agent可能需要面临一个序列的任务。后来，将这一思想扩展到RL设置中，这表明，与外界进行长时间交互的agent必须执行一系列任务。另外，agent可能发现一系列空间上而不是时间上分离的子任务。迁移是任何此类系统的关键组成部分，但终身学习框架比迁移要求更高。首先，传输算法可能合理地关注于一对相关任务之间的传输，而不是试图解释代理可能遇到的任何未来任务。其次，迁移算法通常在新任务开始时被告知，而在终身学习中，可以合理地期望代理在全局MDP(即现实世界)中自动识别新的子任务。
#### 3.5.2 模仿学习(Imitation Learning)
模仿方法的主要动机是让agent通过观察具有类似能力的另一个agent来学习或一个人类执行一项任务。这样的算法试图通过观察外部参与者来学习策略，从而对推断的策略进行潜在的改进。与此相反，我们对迁移学习的定义关注的是agent成功地重用新问题上的内部知识。
#### 3.5.3 人类的建议(Human Advise)
越来越多的研究将人类的建议整合到RL学习者中。例如，一个人可以向agent提供行动建议或通过在线反馈指导代理。利用人类的背景和特定于任务的知识可以显著提高agent的学习能力，但它依赖于一个紧密集成到学习循环中的人，以在线方式提供反馈。相反，这个调查集中于迁移方法，其中一个人不是连续可用的，agent必须自主学习。
#### 3.5.4 塑造(Shaping)
奖励塑造(Reward Shaping)在一个RL上下文通常是指允许agent在一个人工奖励信号而不是R中训练。例如，在Mountain Car任务中，当它接近我们的目标状态时，agent可以获得更高的回报，而不是除了目标状态，在每一个状态都是。然而，如果人类能够计算出这样的奖励，他/她可能已经知道目标位置，这是agent通常不具备的知识。此外，构建的奖励函数必须是一个势函数。如果不是，新MDP的最优策略可能与原MDP不同。塑造的第二个定义遵循斯金纳的研究(斯金纳，1953)，奖励函数随着时间的推移而改变，以指导学习者的行为。这种方法，以及使用静态人工奖励的方法，都是将人类知识注入任务定义中以提高学习效率的方法。
Erez和Smart(2008)提出了第三种定义，即塑造为任何监督的、迭代的、有助于学习的过程。这包括随着时间修改任务的动态，随着时间修改内部学习参数，增加代理可用的操作，以及扩展代理的策略时间范围(例如，在值迭代中所做的)。所有这些方法都依赖于人类智能地来帮助agent完成其学习任务，并且可以利用类似于传输的方法在稍微不同的任务之间成功地重用知识。在讨论迁移时，我们将强调如何成功地重用知识，而不是一个人如何修改任务来达到所需的agent行为，从而提高agent的学习性能。
#### 3.5.5 表示迁移(Representation Transfer)
转移学习问题通常被描述为利用在源任务上学到的知识来改进在相关但不同的目标任务上的学习。Taylor和Stone研究了在具有不同内部表示(例如函数逼近器或学习算法)的agent之间迁移知识的互补任务。允许这种表现形式的变化，为代理设计人员提供了额外的灵活性；如果需要新的表现形式，过去的经验可以迁移而不是抛弃。更重要的好处是，通过学习中途改变表示可以让代理在更短的时间内获得更好的性能。选择一种表现形式通常是解决问题的关键(cf.，肢解棋盘问题，麦卡锡1964，其中人类对问题的内部表现形式大大改变了问题的可解性)，不同的表现形式可能使迁移变得更困难或更不困难。然而，表征选择是RL中的一个普遍难题，对表征选择(或其在迁移效能中的应用)的讨论超出了本文的范围。

## 4. 固定状态变量和动作的迁移方法

![25924e4655c73d2b1ae68d27cdfea1f3.png](:/5862c62389f348a8889bab6d1ba25732)

Selfridge等人(1985)证明：对一系列任务进行训练并重用所学习的函数逼近器所花费的总时间比直接对最困难的任务进行训练要快。通过随时间改变任务的转换函数T来学习平衡车的一根杆子更快。
同样，从简单任务中学习的想法(Asada等人，1994)也依赖于人为学习者构建一组任务。
与其随时间改变任务，不如考虑将任务分解成一系列较小的任务。Singh(1992)使用了一种他称之为合成学习(Compositional learning )的技术来发现如何在一个整体任务中分离时间顺序的子任务。
每个子任务都有不同的开始条件和终止条件，与完整任务相比，单独学习每个子任务要容易得多。只有奖励函数R允许在不同的子任务之间更改，其他MDP组件都不能更改，但是总奖励可以增加。如果问题中的子任务可以通过状态特征识别，那么这些子任务可以通过视觉算法自动识别(Drummond， 2002)。同样，将任务分解为更小的子任务可以提高总回报和渐近性能。由于所使用的视觉算法的限制，这种特殊的方法只直接适用于特征明确定义的子任务。例如，在2D导航任务中，每个房间可能是一个子任务，无法通过的墙壁之间的陡峭值函数梯度很容易识别。但是，如果不同子任务之间的值函数梯度不明显，或者状态空间的子任务区域不是多边形的，那么算法很可能无法自动识别子任务。
在Atkeson和Santamaria(1997)中，再次考虑了只有奖励函数不同的任务之间的迁移。他们的方法通过将转移函数直接应用于目标任务，成功地转移了在源任务中学习到的转换函数的局部加权回归模型。因为他们的模型支持对转移函数进行规划，而不考虑奖励函数，所以他们对跳转开始和总奖励以及渐近性能都有显著的改进。
**接下来的三个方法在不同的任务之间传输部分策略或选项。**
**首先**，Asadi和Huber(2007)让agent在源任务中识别出“在局部形成状态空间轨迹明显更强的‘吸引子’”的状态作为子目标。然后，代理通过一个学习到的动作值函数(称为决策级模型，*decision-level*)学习实现这些子目标的选项(*options*)。第二个操作值函数是评估级(*evolutional-level*)模型，它包含所有动作和整个状态空间。agent仅通过考虑决策级模型来选择动作，但根据需要使用两个模型之间的差异自动增加决策级模型的复杂性。该模型采用分层有界参数*SMDP(Hierarchical Bounded Parameter SMDP)* 表示，使简化模型中最优策略的性能与在初始模型中最优策略的性能有一定的固定界限。实验表明，将学习到的选项和决策级表示方法进行转换，可以使目标任务agent更快地学习具有不同奖励函数的任务。在大约20，000个目标任务状态中，决策级模型中只需要81个不同的状态，因为在从学习到的选项中进行选择时，不需要区分大多数状态。 
**其次**，Andre和Russell(2002)在任务之间转移学习到的子例程，这些子例程类似于选项。作者假设源任务和目标任务具有层次结构。该方法突出了状态抽象与迁移之间的联系；如果可以在这两个任务的状态空间部分之间找到相似之处，那么很可能可以直接传输优秀的局部控制器或局部策略。
**第三**，Ravindran和Barto(2003b)在一个小的、人为选择的源任务中学习相对化(*relativized options*) 的选项。当在目标任务中学习时，向agent提供了这些选项和一组可以应用于选项的可能迁移，以便它们与目标任务相关。代理使用目标和贝叶斯参数估计中的经验来选择要使用哪些转换，从而增加目标任务的总回报。源任务中的学习时间被忽略，但与目标任务的学习时间相比，假设其较小。
接下来，Ferguson和Mahadevan(2006)采用了一种独特的方法来迁移关于源任务结构的信息。原始值函数(Proto-value functions，PVFs)指定了一组不考虑R的正交正规基函数，这些基函数可用于学习动作值函数。在小的源任务中学习PVFs之后，可以将它们转移到另一个离散的MDP，该MDP具有不同的目标或状态空间的小更改。与不使用PVFs相比，使用转移的PVFs可以更快地学习目标任务，并获得更高的总回报。此外，PVF可以扩展到更大的任务。例如，目标迷宫的宽度和高度可以是源迷宫的两倍：R、S和T都按相同的因子缩放。在所有情况下，只计算目标任务时间。
学习PVFs的目标对于一般的RL(尤其是TL)非常有用。直观地说，关于如何在一个领域中最好地学习的高级信息，例如需要进行推理的适当特征，可以很好地跨任务传递。很少有这样的例子，TL算法学习高级知识来帮助agent学习，而不是学习如何操作的低级知识。但是，我们认为这种方法有足够的空间，包括学习其他领域特定的学习参数的方法，如学习速率、函数近似表示等。
Sherstov和Stone(2005)没有通过转移一组基函数来偏置目标任务代理的学习表示，而是考虑如何通过转移适当的**动作集**来偏置代理。
所做的改进：
（1）简化目标任务中的动作集；
（2）状态被分组到类中（抽象）；
（3）利用摄动随机任务
## (Random Task Perturbation， RTP)从单个源任务中创建一系列源任务，因此能生成一个操作集，它将在与源任务不太相似的目标任务中执行得很好。
Lefﬂer等人拓展了他们的工作，就是应用结果/类框架可以显著更快地学习单个任务，并在模拟和物理领域提供了正确性的经验证据。
RTP的概念不仅在本次调查中是独特的，而且在一般情况下，它也可能是一个非常有用的概念。虽然许多TL方法能够从一组源任务中学习，但是没有其他方法尝试自动生成这些源任务。如果一个agent的目标是在一个新的目标任务中尽可能地执行好，那么agent就会尝试在许多源任务上进行训练，即使这些任务是人为的，这也是有道理的。如何最好地生成这样的源任务，使它们最有可能对同一领域中的任意目标任务有用，这是开放研究的一个重要领域。
**渐进RL**(Progressive RL)是一种在难度不断增加的任务之间进行转移的方法，但仅限于离散的MDPs。在学习源任务之后，agent执行反省(*introspection*)，在此过程中，符号学习者根据之前学习的所有任务的q值提取行为规则。RL算法和内省使用不同的状态特征。因此，这两种学习机制在不同的状态空间中学习，其中，符号学习者的状态特征是高层次的，包含了对agent隐藏的信息。当agent在一个新任务中执行操作时，当它第一次到达一个新状态时，它初始化该状态的q值，以便首选学习规则所建议的操作。渐进RL允许agent在一组任务中学习信息，然后将知识抽象为更高层次的表示，从而使agent获得更高的总回报，并第一次更快地达到目标状态。在源任务中花费的时间不计算在内。
最后，Lazaric(2008)证明了**源任务实例(Instance)** 可以在任务之间有效地转移。在学习一个或多个源任务之后，在目标任务中收集一些经验，这些经验可能具有不同的状态空间或转换功能。保存的实例(即观察到的$<s，a，r，s'>$元组)与目标任务中的实例进行比较。根据源任务与目标任务数据之间的距离和对齐程度判断，来自最相似的源任务的实例将被迁移。然后，批处理学习算法同时使用源实例和目标实例来实现更高的回报和跳转。

**区域迁移(*Region Transfer*)** 通过查看每个样本(而不是每个任务)与目标任务的相似性，将该思想向前推进了一步。因此，如果源任务具有与目标更相似的状态空间的不同区域，那么只迁移那些最相似的区域。在这些实验中，在目标任务中花费的训练时间不计入TL算法。

**区域迁移是目前研究的唯一一种明确推理状态空间不同部分任务相似性，然后选择源任务进行转移的方法**。在目标任务具有与一个或多个源任务相似的状态空间区域，以及与其他源任务相似(或与任何源任务都不相似)的其他区域的域中，区域转移可能会提供显著的性能改进。因此，该方法为在线测量和利用任务相似性提供了一种独特的方法。这种方法很可能会影响未来的迁移方法，并且是完成第2部分第1步的一种可能的方法：给定一个目标任务，如果存在要传输的源任务，则选择适当的源任务。

综上所述，这些TL方法表明，在任务之间有效地迁移许多不同类型的信息是可能的，而且任务之间存在各种差异。值得再次强调的是，许多TL方法可以与其他加速方法相结合，比如奖励形成(Reward Shapping)方法，或者与其他转移方法相结合。例如，当迷宫任务之间的转移，在源任务中基函数可以学到(弗格森和马哈，2006)，在一组额外生成的源任务中训练之后，一组待迁移的动作能被选择出来，然后不同的源任务的部分可以利用来学习目标任务(Lazaric，2008)。第二个例子是，从一个简单的源任务开始，通过修改转换函数(Selfridge et al.， 1985)和开始状态(Asada et al.1994)随着时间的推移对其进行更改，同时学习选项(Ravindran和Barto， 2003b)，直到学习了一个困难的目标任务。通过检查源任务和目标任务之间的差异，以及使用什么基础学习方法，RL实践者可以选择一个或多个TL方法来应用于他们感兴趣的领域。然而，在缺乏转移效率的理论保证的情况下，任何TL方法都有可能是有害的，如9.2节中进一步讨论的。

## 5.多任务学习方法

![8961bd8591c64a22aa081937713ef14d.png](:/37e6a6805e9c4bcfa705f31d33630e4a)

本节讨论源任务和目标任务具有相同状态变量和动作的场景。然而，这些方法(见表4，从表1)是显式的MTL，并且这部分的所有方法在这一节中被设计为使用多个源任务(参见图7)。在学习一个新的目标任务中，一些方法利用已经经历的所有源任务，其他方法可以选择之前经历过的任务的子集。哪种方法最合适取决于关于任务分布的假设：如果任务足够相似，那么所有过去的经验都是有用的，也就没有必要选择子集。另一方面，如果任务的分布是多模态的($multi-modal$)，那么从所有任务的转移很可能是次优的。这些方法都没有考虑在源任务中学习所花费的时间，因为主要考虑的是从未知(但固定)的MDPs分布中随机选择的下一个任务的有效学习。

**可变奖励层次强化学习**(*Variable-reward hierarchical reinforcement learning*)假设学习者将在一系列任务上进行训练，这些任务除了不同的奖励权重(*reward weight*)外是相同的。奖励权重定义了通过奖励特征的线性组合分配多少奖励。作者为给定的一组任务向agent提供*奖励特征*。例如，在实时战略领域中，不同的任务可能会改变奖励的特征，比如从收集金币或伤害敌人中获得的收益。然而，不清楚有多少感兴趣的领域具有奖励特征，这些特征在每个任务开始时提供给agent。使用分层RL方法，子任务策略被学习。当遇到一个新的目标任务时，agent将初始策略设置为最相似的源任务的初始策略，由先前观察到的奖励权重向量的点积决定。Agent然后使用一个$\epsilon-greedy$行动选择方法在每个级别的任务层次结构来决定是否使用最著名的子任务策略或探索。有些子任务，比如导航，永远不需要为不同的任务重新学习，因为它们不受奖励权重的影响，但是任何次优子任务策略都会得到改进。因为当agent经历更多的任务时，相对于不迁移的学习任务，每个新目标任务的总报酬增加。

Perkins and Precup(1999)提出了一个不同的问题公式，其中过渡函数$T$在达到目标后可能发生变化。当达到目标时，agent将返回到起始状态，不知道转换函数是否或如何更改，但是它知道$T$是从某个固定分布中随机抽取的。为agent提供了一组人为编码的**选项**，这些选项有助于学习这组任务。随着时间的推移，agent通过这些选项学习一个准确的操作值函数。因此，可以通过一组任务学习单个动作值函数，从而允许agent更快地实现具有新转换函数的任务的目标。

Foster和Dayan(2004)的目的不是迁移选项，而是在源任务中**识别子任务**，并在目标任务中使用这些信息，这与Singh(1992)的动机类似。允许任务在目标状态的位置上有所不同。当在源任务中学习最优值函数时，**期望最大化算法**(*expectation-maximization*)在所有学习的任务中识别不同的“片段”或子任务。一旦了解了这些片段，就可以使用它们来增强agent的状态。每个子问题都可以独立学习；当遇到一个新的任务时，大部分的学习已经完成了，因为大多数子问题没有改变。碎片式学习对平滑学习者(例如TD)和显式的分层学习者都有效，以提高启动和总奖励。

**概率策略重用**(*Probabilistic policy reuse* )也考虑任务的分布，其中只有目标状态不同，但在合适的源任务选择方面，它是最鲁棒的MTL方法之一。虽然该方法允许单个目标状态在任务之间有所不同，但它要求S、a和T保持不变。如果新学习的策略与现有策略有显著不同，则将其添加到策略库中。当代理被放置在一个新任务中时，在每一个时间步上，它可以选择：利用一个已学习的源任务策略，为目标任务利用当前的最佳策略，或者随机探索。如果agent的库中有多个已学习的策略，那么它将在策略之间进行概率选择，以便随着时间的推移，将更频繁地选择更有用的策略。虽然此方法允许策略的概率混合，但也可以将过去的策略视为可执行的选项，直到满足某些终止条件为止，类似于前面讨论的许多方法。通过比较混合过去的策略并将其作为选项处理的相对好处，可以更好地理解这两种方法在什么时候最有用。

构建**显式策略库**的想法很可能在未来的TL研究中非常有用，特别是对于训练大量具有较大质量差异(因此学习行为也非常不同)的源任务的agent来说。虽然其他方法也从多个源任务中单独记录信息，Fernandez和Veloso明确地解释了库的原因。除了对存储的信息量进行推理之外，作为源任务的数量和类型的函数，了解需要多少目标任务样本才能选择最有用的源任务将非常有用。

与概率策略重用(有选择地从单个源任务传输信息)不同，Tanaka和Yamamura(2003)提出**收集所有以前任务的统计信息，并使用这些混合的知识更快地学习新的任务**。具体地说，学习者要跟踪在所有任务中观察到的每一对(s，a)动作值的平均值和偏差。当agent遇到新任务时，它将初始化动作值函数，以便将每对(s，a)设置为该对的当前平均值，这相对于不知情的初始化来说是一个优势。当agent通过Q-learning和优先扫描来学习目标任务时，agent使用状态的q值的标准差来设置TD备份的优先级。如果当前的q值远远低于该(s，a)对的平均值，那么应该更快地调整它的值，因为它可能是不正确的(因此应该在影响其他q值之前进行纠正)。此外，另一项解释个体试验中差异的项被添加到优先级；在特定试验中经常波动的q值很可能是错误的。实验表明，将该方法应用于具有不同转换函数的离散任务集，可以显著提高跳跃启动和总报酬。

接下来的两种方法考虑**贝叶斯MTL代理如何有效地学习先验**。
首先，Sunmola和Wyatt(2006)介绍了两种方法，它们使用源任务中的实例来设置贝叶斯学习者的先验。这两种方法都**使用以前的实例作为先验类型来约束目标任务转换函数的概率**。第一种方法使用工作先验生成可能的模型，然后根据目标任务中的数据对模型进行测试。第二种方法是将概率摄动法(Probability Perturbation Mathod)与观测数据相结合，对前一种方法生成的模型进行改进。初始实验表明，如果agent对目标所属类的先验分布有一个准确的估计，则可以提高跳跃启动和总报酬。其次，Wilson等人(2007)考虑在**分层贝叶斯RL环境中学习**。为贝叶斯模型设置先验通常是困难的，但在这项工作中，先验可能从以前学习的任务中转移过来，显著提高了学习速度。此外，该算法可以处理具有相似模型参数的MDPs的“类”，并在引入新的MDP类时进行识别。然后，可以将新类添加到层次结构中，并学习不同的优先级，而不是强制MDP适应现有的类。目标状态的位置和参数化的奖励函数可能因任务的不同而不同。在后续任务的学习中，总的奖励显示出明显的性能改进，跳跃启动也有一些改进。

虽然贝叶斯方法在分类任务之间的迁移(Roy and Kaelbling， 2007)和非迁移RL (Dearden et al.， 1999)已被证明是成功的，但只有上述两种方法在RL迁移中使用贝叶斯方法。学习者的偏置在所有机器学习环境中都很重要。然而，贝叶斯学习使得这种偏置是显式的。通过相似任务的转移来设置偏置可能是一个非常有用的启发式方法——我们希望开发更多的迁移方法来初始化来自过去任务的贝叶斯学习者。

Walsh等人(2006)观察到，“**决定在环境之间传输什么知识可以理解为确定一组源[任务]的正确状态抽象方案，然后将这种压缩应用于目标[任务]**“。他们建议的框架解决了一组MDPs，从解决方案中构建抽象，提取相关特征，然后将基于特征的抽象函数应用于新的目标任务。利用不同状态空间和奖励函数的任务进行的简单实验表明，使用MTL可以减少学习目标任务的时间。通过构建五个类型的状态定义抽象(如李et al . 2006年)中定义的，他们给出理论结果表明，当源任务的数量大(相对于不同的任务之间的差异)，四种不同的抽象使用Q-learning能够保证产生在目标任务中产生最优政策。

与Walsh等人(2006)类似，Lazaric(2008)也发现了要迁移的特征。与上面所有的论文中所述的顺序学习任务不同，我们可以考虑**并行学习不同的任务，并使用共享的信息来更好地学习任务，而不是单独学习**。具体来说，Lazaric(2008)使用拟合q迭代(*Fitted Q-iteration*)的批处理方法学习了一组具有不同奖励函数的任务。通过利用多任务特征学习算法，该问题可以被表述为一个联合优化问题，在所有任务的观测数据中寻找最佳特征和学习参数。实验表明，该方法可以提高总报酬，帮助agent忽略不相关的特征(即，不提供有用信息的特征)。此外，由于可能学习更好的表示，相对于孤立的学习任务，渐进性能也可能得到改善。

本节的工作，如表1第二节所总结的，明确假设agent经历的所有的MDPs都来自于相同的分布。单个分布中的不同任务在原则上可能具有不同的状态变量和操作，未来的工作应该研究何时允许这种灵活性是有益的。

## 6. 在具有不同状态变量和动作的任务之间迁移任务不变的知识
与前两个部分不同，本节讨论允许源任务和目标任务具有不同状态变量和操作的方法(参见图8和表5中的方法)。这些方法将问题公式化，因此不需要在任务之间进行显式映射。相反，当操作或状态变量发生变化时，代理对MDP的抽象是不变的。
![a43d52dbaa49342ef9376b1aac2a25da.png](:/9e6a8ea74fa846ee8e6862537f4ed767)
例如，Konidaris和Barto(2006)将标准RL问题分为代理空间(*Agent-space*)和问题空间(*Problem-space*)表示。代理空间由agent的能力决定，这些能力是固定的(例如，物理传感器和执行器)，尽管这样的空间可能不是马尔可夫式的。另一方面，问题空间可能会在源问题和目标问题之间发生变化，并被认为是马尔科夫式的。作者的方法是在学习源任务的同时在线学习agent空间中的成型奖励。如果以后的目标任务具有类似的奖励结构和动作集，那么习得的成型奖励将帮助agent实现一个跳跃启动和更高的总奖励。例如，假设代理的一个传感器测量它与特定重要状态(例如位于目标状态附近的信标)之间的距离。即使在没有环境奖励的情况下，当描述其到信标距离的状态变量减少时，agent也可以学习一个赋值奖励的成形奖励。作者假定没有新的动作(即那些不在源任务问题空间的动作)，但如果可以将任何新的状态变量从新的问题空间映射到熟悉的代理空间，则可以处理任何新的状态变量。此外，作者承认，这种转移必须在与奖励相关的(*reward-linked*)任务之间进行，其中“每个环境中的奖励函数始终将奖励分配给跨环境中相同类型的交互。”确定一系列的任务是否符合这个标准留给未来的工作。

在后来的工作中(Konidaris和Barto， 2007)，作者假设了 **“预先指定的突出事件”的知识，这使得学习选项易于处理**。虽然可以在不需要指定事件的情况下学习选项，但是本文的重点是如何使用这些选项，而不是选项学习。具体地说，当代理实现这些子目标之一时，比如打开一扇门或穿过一扇门，它可能会学习在将来再次实现该事件的选项。正如预期的那样，问题空间选项加快了学习单个任务的速度。更有趣的是，当代理对一系列任务进行训练时，代理空间和问题空间中的选项都会显著增加目标任务的启动时间和总回报(学习源任务的时间不考虑)。作者认为，在源任务和目标任务不太相似的情况下，代理空间选项可能比问题空间选项更可移植——实际上，只有当源任务和目标任务非常相似时，问题空间选项才可移植。

在我们看来，代理和问题空间是应该进一步探索的想法，因为它们可能会产生额外的好处。特别是在物理agent的情况下，可以直观地看到代理传感器和执行器将是静态的，从而可以方便地重用信息。特定于任务的项，例如特征和动作，可能会发生变化，但是如果代理已经了解了关于其不变的代理空间的一些信息，那么学习起来应该更快。

如果将迁移应用到**游戏树**中，动作和状态变量的更改可能问题较少。Banerjee和Stone(2007)能够通过专注于这种更抽象的表达方式在游戏之间进行迁移。例如，在实验中，学习者确定了叉的概念，即无论对手下一步怎么走，玩家都能在接下来的回合中获胜的状态。在对源任务进行训练后，对这些特征的源任务数据进行分析，然后根据源任务数据为给定的特征设置值，将游戏树的这些特征用于各种目标任务中。这种分析主要关注动作对游戏树的影响，因此描述源游戏和目标游戏的动作和状态变量可以不同，而不需要任务间映射。源任务时间是不考虑的，但是跳跃启动后、总奖励和渐近性能都通过转移得到了改进。虽然本文的实验只使用了时序差分学习，但这种方法很可能适用于其他类型的学习者。

Guestrin等人(2003)在他们称之为关系MDP(*Relational MDP*)的规划上下文中研究了一个类似的问题。**不是学习标准的值函数，而是在源任务中计算每个agent的以agent为中心的值函数，强制给定类类型的所有代理都具有相同的值函数**。但是，定义这些类值函数是为了使它们独立于任务中的agent数量，从而允许它们直接用于具有更多(或更少)代理的目标任务。在目标任务中没有进一步的学习，但是迁移的值函数比作者提供的人为编码的策略执行得更好，尽管有额外的友好和敌对代理。然而，作者注意到，该技术在具有“许多对象之间强大且持续的交互”的异构环境或领域中不会表现得很好。

**关系强化学习**(*Relational Reinforcement Learning*)也可用于有效的迁移。RRL学习者通常通过构造一阶规则以命题形式推理状态，而不是将状态作为来自Agent传感器的输入进行推理。学习者可以很容易地抽象出特定的目标身份以及环境中的目标数量；简化了不同数量目标的任务之间的迁移。例如，Croonenborghs等人(2007)首先学习了使用RRL的源任务策略，所学习的策略用于创建状态-动作对的样本，然后用于构建关系决策树。对于给定的状态，此树预测策略将执行哪些动作。最后，挖掘这些树以生成*关系选项(Relational Option)*。这些选项直接用于目标任务，假设任务非常相似，不需要转换关系选项。作者考虑了三对源/目标任务，其中源任务中学习到的关系选项直接应用于目标任务(只有任务中的目标数量可能发生变化)，并且学习在跳转启动、总奖励和渐进性能方面得到了显著改善。

使用RRL进行迁移的其他工作介绍了TGR算法，这是一种关系决策树算法。TGR增量地构建一个决策树，其中内部节点使用一阶逻辑去分析当前状态，并且树的叶子包含动作值。该算法利用四种树重构算子有效地利用可用内存，提高了样本效率。首先对一个简单的源任务进行训练，然后对一个相关的目标任务进行训练，从而减少目标任务时间和总时间。跳跃启动、总奖励和渐近性能似乎也通过迁移得到改善。

RRL是迁移学习中一个特别有吸引力的范式。在RRL中，虽然为了达到最佳(甚至可接受的)性能水平，可能需要进行额外的训练，但是agent通常可以在具有附加目标的任务中工作，而不需要再形成它们。当可以将感兴趣的域作为RRL任务来构建时，具有不同数量对象或代理的任务之间的迁移可能会相对简单。

与RRL的动机类似，可以构造一些学习问题，以便代理在高级动作之间进行选择，而不管要推理的目标的数量。Sharma等(2007)将基于案例的推理与RL相结合，在**基于案例的强化学习者(*CAse-Based Reinforcement Learner ，CARL*)** 中，一个多层次的体系结构包括三个模块：规划器、控制器和学习者。战术层使用学习者在高级(*High-level*)动作之间进行选择，这些高级动作独立于任务中的目标数量。案例的索引是：高级状态变量(同样独立于任务中的目标数量)、可用的动作、动作的q值，以及该案例对前面时间步骤的累积贡献。当前情况与过去案例的相似性是由欧几里得距离决定的。由于定义了状态变量和动作，以便任务中的对象数量可以更改，所以源任务和目标任务可以有不同数量的目标(在样本域中，作者在源任务和目标任务中使用了不同数量的玩家和对手部队)。花费在学习源任务上的时间没有计算在内，但是目标任务的性能是用跳转启动、渐近增益(与平均报酬超过学习的改进相关的度量)和总体增益(基于累积的总报酬的度量)来度量的。

总之，本节研究的方法都允许在具有不同状态变量和动作的任务之间进行迁移，以及迁移函数、状态空间和奖励函数。通过在以代理为中心的空间中构建任务，将领域限制为游戏树，或者使用一种从数量可变的目标中推断的学习方法，可以相对容易地在任务之间迁移知识，因为问题表示不会从学习者的角度发生变化。一般来说，并不是所有的任务都可以按照本节中介绍的TL方法所做的假设来制定。

## 7. 用于在不同动作和状态表示之间迁移的显式映射
这部分的调查集中在比以前讨论的更加灵活的一组方法，因为他们允许源和目标任务的状态变量和可用的动作是不同的(见表6和图9)。在这一节中使用所有方法类间(*inter-task*)映射，使之能在前面部分中的方法无法解决的任务之间迁移。请注意，由于状态变量和动作的变化，R、S和T，所有技术上也发生了变化(它们是在动作和状态变量上定义的函数)。然而，正如我们在下面详细说明的，一些方法允许在任务之间对奖励函数进行重大更改，而大多数方法则不允许。

在Taylor等人(2007a)中，作者假设源任务和目标任务之间的映射被提供给学习者。学习者首先使用值函数学习方法训练源任务，在目标任务中开始学习之前，目标任务中每个状态的每个动作值都是通过已学习的源任务值初始化的。**实验表明，通过在具有不同状态变量和动作的任务之间进行值函数传递可以显著提高速度。** 此外，研究了不同的值函数迁移方法，成功地使用了不同的函数逼近器，并演示了多步迁移(即转移从一个任务，任务B C)。这个TL方法表明，当面对一个困难的任务，对源任务或一组源进行整体一次训练然后把知识迁移到目标任务，这样会更快。作者没有提供关于他们的方法有效性的理论保证，而是假设了他们的TL方法在什么情况下会和不会表现得很好，并提供了他们的方法不能通过迁移减少训练时间的例子。

在随后的工作中，Taylor等人(2007b)在具有不同状态变量和动作的任务之间**迁移整个策略，而不是动作值函数**。首先通过源任务中的遗传算法学习一组策略，然后通过任务间映射进行迁移。此外，还引入了部分任务间映射，这可能使人类更容易在许多领域凭直觉感知。具体地说，那些源任务中的动作和状态变量与目标中的操作和状态变量“非常相似”的进行映射，而不映射目标任务中的新状态变量和状态变量。策略使用其中一个任务间映射进行转换，然后用于在目标任务中播种学习算法。与前面的工作一样，这种TL方法可以成功地减少目标任务时间和总时间。

稍后，Taylor等人(2008b)再次考虑了动作不同、状态变量不同以及学习者可以使用任务间映射的任务对。在这项工作中，作者允许通过**迁移实例**在模型学习方法之间进行转移，这在本质上类似于Lazaric(2008)。拟合R-MAX (Jong and Stone， 2007)是一种基于实例的模型学习方法，能够在连续状态空间中学习，作为基本的RL方法，将源任务实例转移到目标任务中，以更好地逼近目标任务的模型。在一个简单的连续域中进行的实验表明，转移可以提高目标任务的跳跃启动、总报酬和渐近性能。

另一种迁移方式是通过**学习建议或偏好**。Torrey等人(2005)通过识别具有比其他可用操作更高q值的操作，自动从源任务中提取此类建议。这样的建议通过人为提供的任务间映射映射到目标任务，作为给目标任务学习者的首选项。在这项工作中，q值通过支持向量回归学习，然后基于偏好知识的核回归(KBKR) (Maclin et al.， 2005)将建议作为软约束添加到目标中，为不同状态下的不同动作设置相对偏好。即使源任务具有不同的状态变量和动作，目标任务学习者也可以成功地利用这些建议，从而减少目标任务的学习时间。此外，任务的奖励结构可能存在很大差异：他们的实验使用的源任务的奖励是基于episode长度的无界得分，而目标任务的奖励是二进制的，这取决于agent是否达到了目标状态。源任务时间不考虑，目标任务学习在总奖励和渐近性能方面略有改善。

后来的工作(Torrey et al.， 2006)通过使用**归纳逻辑编程(*Inductive logic programming， ILP*)来识别对源任务中的Agent有用的技能**，从而改进了这种方法。检查源任务中Agent的跟踪，并提取正面和负面示例。通过观察哪个动作被执行、产生的结果、动作的q值以及其他可用动作的相对q值，可以确定积极和消极示例。使用ILP引擎Aleph (Srinivasan， 2001)通过$F_{1}$分数(精确度和回归的调和平均值)提取技能。然后，人类将这些技能映射到目标任务中，通过KBKR改进学习。源任务时间不计入目标任务时间，跳转启动可能会得到改善，总奖励也会得到改善。源任务和目标任务在状态变量、操作和奖励结构方面又有所不同。作者还展示了除了在源任务中生成的建议外，如何容易地合并人为提供的建议。最后，作者通过实验证明，给学习者错误的建议只是暂时的有害，而且随着时间的推移，学习者可以“忘记”错误的建议，这可能对最小化负迁移的影响很重要。

Torrey et al.(2007)进一步将他们的技术**推广到迁移策略**，迁移策略可能需要将多个技能组合在一起，并定义为有限状态机(Fnite-state machine， FSM)。算法的**结构学习**(*structure learning*)阶段分析源任务数据，找出区分成功游戏和不成功游戏的动作序列(例如，是否达到了目标)，并将这些动作组合成一个FSM。第二阶段，**规则集学习(*ruleset learning*)**，基于状态特征学习策略中的每个动作何时应该执行，以及FSM何时应该过渡到下一个状态。对于Aleph，源任务的经验又一次被分为积极序列和消极序列。一旦通过人工提供的映射将策略重新映射到目标任务，就可以使用它们向目标任务学习者演示策略。目标任务学习者不是随机探索，而是在前100个episode中执行转移策略，从而学会估计被迁移策略所选择的动作的q值。在这个演示阶段之后，学习者从MDP的动作中进行选择，而不是从高层次的策略中进行选择，并可以学习如何在已经迁移的策略上改进。实验表明，当源任务和目标任务具有不同的状态变量和动作时(源任务时间再次折算)，策略迁移显著提高了目标任务的跳跃启动和总奖励。

类似于策略迁移，Taylor and Stone (2007b)使用RIPPER (Cohen， 1995)**学习规则，总结一个已经学习好了的源任务策略**。然后，通过人工编码的任务间映射迁移规则，使它们能够应用于具有不同状态变量和操作的目标任务。然后，目标任务学习者可以通过将这些规则合并为一个额外的动作来引导学习，本质上是添加一个始终存在的选项“采取源任务策略建议的动作”，从而改进了跳跃启动和总奖励。通过使用规则作为两个任务之间的中介，作者认为源任务和目标任务可以使用不同的学习方法，有效地消除两个学习者之间的耦合。与Torrey et al.(2007)的相似之处包括初始性能的显著改善和没有自动处理尺度差异的规定。这些方法的主要区别在于如何将建议融入目标学习者和规则学习者的选择中。

此外，Taylor和Stone (2007b)证明了**域间(*Inter-domain*)迁移**可能的。本文的两个源任务是离散的，完全可观测的，一个是确定性的。然而，目标任务具有连续的状态空间、部分可观测性和随机动作。由于源任务所需的时间少了几个数量级，所以总时间大致等于目标任务时间。我们过去的工作使用“域间迁移”这个术语来描述定性不同的域之间的转移，例如棋盘游戏和足球模拟之间的转移。然而，这个术语没有很好地定义，甚至在社区中也没有达成一致。例如，Swarup和Ray(2006)使用术语“跨域迁移”来描述具有不同数量布尔输入和单个输出的分类任务之间神经网络结构的重用。然而，我们希望研究人员能够继续改进转移方法，这样他们就可以有效地转移非常不同的任务，就像人类在非常不同的领域之间转移高层次思想的方式一样。

本文讨论了低层次和高层次知识迁移的实例。例如，学习一般规则或建议可能被视为相对较高的层次，而迁移特定的q值或观察到的实例则是相当特定于任务的。我们的直觉是，当在非常不同的任务之间转换时，更高层次的知识可能更有用。例如，为跳棋游戏学习的Q值不太可能转移到国际象棋中，但是叉(fork)的概念可以很好地转移。然而，这一点还没有得到明确的证明，也没有一种定量的方法来对知识进行高低层次的分类。我们希望未来的工作将证实或否定这一假设，并产生指导方针，以确定不同类型的知识转移何时是最合适的。

本节中的所有方法都使用某种类型的任务间映射，以允许具有非常不同规范的MDPs之间进行传输。虽然这些结果表明迁移可以提供一个显著的好处，但他们假设映射是提供给学习者的。下一节将讨论自动学习此类任务间映射的方法。

## 8. 学习任务映射
到目前为止所考虑的传输算法都假定提供了任务之间的人为编码映射，或者不需要映射。在本节中，我们考虑如何在难探索问题的任务中习得任务之间的映射，这样源任务的知识可能在一个新的不同状态变量和动作的目标任务中被利用(参见图10和最后一组表1)。请注意，在这一节中，除了一个传输方法外，其他都有N/A的方法。本节的论文介绍了映射学习方法，并利用已有的方法验证了映射的有效性。

TL研究目前面临的一个挑战是**减少向学习者提供的有关源任务和目标任务之间关系的信息量**。如果一个人在根据一系列任务指导学习者，那么这些任务之间的相似性(或类比)很可能是由人类的直觉提供的。然而，如果要在自主环境中取得成功，学习者必须首先确定两个任务如何(以及是否)相关，只有这样，agent才能利用其过去的知识在目标任务中学习。如果要在没有人工输入的情况下转移代理，学习任务之间的关系是至关重要的，这要么是因为人类在循环之外，要么是因为人类无法提供任务之间的相似性。本节中的方法主要不同在于必须提供什么信息。在光谱的一端，Kuhlmann和Stone(2007)假设给出了R、S和T的完整描述，而在另一端，Taylor等人(2008c)只从通过环境相互作用收集的经验中学习映射。

给出一个完整的游戏描述(即MDP的完整模型)，Kuhlmann和Stone(2007)通过对游戏进行分析，生成了一个**规则图(*Rule Graph*)，一个确定性的、完全信息游戏的抽象表示**。学习者首先在一系列源任务游戏中训练，存储规则图和已经学习的值函数。当一个新的目标任务呈现给学习者时，它首先构造目标任务的规则图，然后试图找到一个具有同构规则图的源任务。学习者假设提供了一个转移函数，并使用基于值函数的学习来估计游戏后面状态的价值。只需要在源任务和目标任务之间映射状态变量，这正是通过图匹配找到的映射。对于目标任务中的每个状态，通过查找源任务中对应状态的值来设置初始q值。考虑了三种类型的迁移：直接，它不需要修改就可以复制状态值；逆，指目标颠倒或角色互换；平均，复制一组q值的平均值，并可用于不同大小的板。源任务时间被忽略，但是jumpstart和total reward都可以在目标任务中得到改进。

前面的工作假定转移函数的全部知识。更一般的方法可以假设代理只定性地理解转移函数。例如，**定性动态贝叶斯网络(QDBNs)(*qualitative dynamic Bayes networks，QDBNs*)** 总结了行为对状态变量的影响，但并不精确(例如，它们不能作为规划的生成模型)。如果向agent提供QDBNs，则图映射技术可以在两个任务中自动找到动作和状态变量之间的映射，且计算成本相对较低。作者表明，映射可以自主学习，有效地支持具有不同状态变量和操作的任务之间的值函数传递。然而，QDBNs是否可以从经验中学习，而不是人为编码，仍然是一个悬而未决的问题。

接下来的三种方法**假设如何使用状态变量来描述多玩家任务中的目标的知识**。例如，agent可能知道一对状态变量描述了“到队友的距离”和“从队友到标记的距离”，但是agent并不知道状态变量描述的是哪个队友。

首先，Soni和Singh(2006)为代理提供了一系列可能的状态转换和一个任务间动作映射。对于目标任务变量到源任务变量的每个可能映射，都有一个这样的转换$X$。学习源任务后，代理的目标是学会正确的转换：在每个目标任务状态s，代理可以随机探索目标任务的行为，也可以选择采取的行动$\pi_{source}(X (s))$。这种方法与Fernandez和Veloso(2006)的动机类似，但在这里，作者正在学习在可能的映射之间进行选择，而不是在以前可能的策略之间进行选择。随着时间的推移，代理使用Q-learning来选择最佳状态变量映射，并学习目标任务的动作值。使用此方法时，跳跃启动、总奖励和渐近性能都略有改进，但其有效性将严重依赖于任何源和目标任务之间**可能映射的数量**。

其次，AtEase (Talvitie and Singh， 2007)也生成了一些可能的状态变量映射。**再次假设动作映射，目标任务学习者将每个可能的映射视为多臂强盗的手臂**(Bellman， 1956)。作者证明了他们的算法的学习时间与可能映射的数量成正比，而不是与问题的大小成正比：“在T的时间多项式中，(该算法)实现的实际回报接近于最优专家的渐近回报，其混合时间最多为T。这种方法专注于有效地选择所提出的状态变量映射，不允许目标任务学习。

第三，Taylor等人(2007b)稍微放松了这些假设，他们指出，通过利用分类技术，可以同时学习动作和状态变量映射，尽管它再次依赖于预先指定的状态变量分组(即，知道“与队友的距离”指的是一个队友，而不是哪个队友)。使用记录的源任务数据训练动作和状态变量分类器。例如，源任务代理在与环境交互时记录$s_{source}、a_{source}和s'_{source}$元组。训练一个动作分类器，使得源任务的每个目标都有$C(s_{source，object}，s'_{source，object})=a_{source}$。稍后，目标任务agent再次记录$s_{target}、a_{target}、s'_{target}$元组。然后，动作分类器可以再次用于对每个目标任务对象的元组进行分类：$C(s_{target，object}，s'_{target，object}) = a_{source}$，这样的分类将指示$a_{target}$和$a_{source}$之间的映射。准确分类所需数据相对较少；目标任务中需要学习的样本数量远远超过映射学习步骤中使用的样本数量。虽然得到的映射并不总是最优的，但它们确实可以有效地减少目标任务训练时间和总训练时间。

主算法(*Master*)的设计是为了进一步放松的知识要求：不需要状态变量分组。MASTER的核心思想是保存有经验的源任务实例，从一小组经历过的目标任务实例构建近似的转换模型，然后通过测量目标-任务模型在源任务数据上的预测误差，离线测试可能的映射。这种方法以牺牲较高的计算复杂度为代价，特别是随着状态变量和操作的数量的增加而提高了样本效率。该方法使用穷举搜索来寻找任务间的映射，以最小化预测误差，但是可以结合更复杂的搜索方法(例如启发式)。实验表明，学习的任务间映射能够有效地提高跳跃启动和总奖励。通过一系列实验，说明了该算法如何通过选择最能减小离线预测误差的源任务来辅助源任务选择。MASTER的主要贡献是证明自主迁移是可能的，因为该算法可以自主地学习任务间映射，然后可以被本调查的前一节(第7节)讨论的任何TL方法所使用。

总之，调查的最后一部分讨论了几种能够学习具有不同数据量的任务间映射的方法。尽管所有这些方法都对提供给学习者的知识量或源任务与目标任务之间的相似性做出了一些假设，但这些方法都是实现完全自主迁移的重要一步。

在增加自主权方面，本节中的方法排列松散。通过学习任务间映射，这些算法试图使TL代理能够在不需要人工干预的情况下对新任务使用过去的知识，即使状态变量或动作发生了变化。然而，问题仍然是，完全自主迁移在实践中是否有用。具体地说，如果对可能遇到的目标任务的类型没有限制，为什么人们会期望在学习一个可能遇到的任务时，甚至是在学习大多数可能遇到的任务时，过去的知识(一种偏置)会有用呢？这个问题与TL算法识别任务何时相似以及何时可能发生负迁移的能力直接相关，这两个问题将在下一节中详细讨论。

## 9. 开放问题
虽然近年来RL的迁移学习取得了显著的进步，但仍有许多有待解决的问题。本节列出了一些我们认为特别重要的问题。9.1节讨论了调查中方法可能被扩展的方法，并强调了一些最有希望用于未来工作的方法。9.2节接着讨论了负迁移问题，这是目前最令人不安的公开问题之一。最后，9.3节提出了一组作者认为对TL领域最有帮助的可能的研究方向。

### 9.1 潜在的增强
我们分类法中一个明显的缺陷是缺乏模型学习方法。由于模型学习算法通常比非模型学习算法具有更高的样本效率，所以当与这种有效的RL方法相结合时，TL可能会对样本复杂度产生较大的影响。此外，当在源任务中学习环境的完整模型时，当目标任务学习者遇到模型与目标任务之间的差异时，可能会明确地推断如何细化或扩展模型。

正如在第5节中提到的，**迁移是在贝叶斯设置中设置先验的一种吸引人的方法**。在MTL环境中，这可以准确地学习任务分配中的先验知识，从而使学习者更好地避免负迁移。迁移学习的主要好处之一是能够使学习者产生偏置，这样他们就可以用更少的数据找到更好的解决方案；通过贝叶斯先验将这些偏差显式化，可能会允许更有效(且人类可理解)的迁移方法。虽然扩展当前方法来处理复杂任务(可能具有复杂的分布层次结构)可能会有困难，但贝叶斯方法似乎特别适合于迁移。

**自动修改源任务的想法(cf.， RTP Sherstov和Stone 2005， Kuhlmann和Stone 2007提出)还没有被广泛采用**。然而，在目标任务学习性能至关重要的环境中，这种方法有可能提高转移效率。通过开发允许对一系列自动生成的变体进行训练的方法，TL代理可能能够自主地进行训练，并获得可用于新任务的经验。这种方法在多任务学习环境中特别有用，在这种环境中，代理可以利用一些关于它将来看到的目标任务分布的假设。

**在本次调查中，没有一种迁移方法能够明确利用任何关于任务之间奖励函数变化的知识，而且人类可能特别容易识别奖励函数的定性变化。** 例如，如果已知目标任务奖励是源任务奖励的两倍，那么值函数方法就有可能利用这一背景知识自动修改源任务值函数，从而提高学习效果。作为第二个示例，考虑一对任务，其中目标状态从状态空间的一边移动到另一边。虽然可以重用学习到的转换信息，但是需要对策略或价值功能进行重大更改，以适应新的奖励函数。除了$A$和状态变量的变化外，还可以扩展任务间映射来考虑任务之间R的变化。

**来自理论修正(*Theory Revision*)(也是理论改进，*Theory Reﬁnement*)的思想可能有助于实现任务间映射的自动构建。** 例如，许多方法初始化目标任务代理，使其q值类似于源任务代理中的q值。如果目标任务q值足够接近最优q值，那么相对于不使用转移，学习得到了改进，那么转移就可能成功(Taylor et al.， 2007a)。在某些情况下，对知识进行语法( *syntactic*)更改会产生更好的迁移。例如，如果目标任务的奖励函数与源任务函数相反，那么直接迁移q值将远远不是最优的。然而，TL算法可以识别逆关系可以更适当地使用源任务知识(如初始化它的行为，这样$\pi_{target} (s_{target}) \neq \pi_{source}(\chi_{X}(s_{target}))$。

对于成功的迁移应用，代理可能有两个明显的好处。首先，转移可以帮助改进代理的探索，使其更快地发现高值的状态。其次，转移可以帮助偏置代理的内部表示(例如，它的函数逼近器)，以便它可以更快地学习。今后的工作必须更好地区分这两种影响；分离这两个贡献应该可以更好地理解TL的好处，并为将来的改进提供途径。

在讨论的34种转移方法中，只有5种试图发现内部学习参数(例如，适当的特征或学习率)，以便更有效地学习同一领域的未来任务。其他的“**元学习**
(*meta learning*)”方法很可能是有用的。例如，可以学习使用合适的函数逼近器、有利的学习率，甚至是最合适的RL方法。尽管在MTL环境中可能更容易完成，但如果对任务相似性有足够强的假设，这种元学习也可能在迁移中实现。关于选择RL方法的最佳方法和特定领域的学习参数设置的多种启发式方法都存在，但通常以一种特殊(ad hoc)的方式选择这些设置。在设置这些参数时，迁移可能会提供帮助，而不是依赖于人类的直觉。

第8节讨论了学习一个任务间映射，其动机是这样的映射可以实现自主迁移。然而，目前还不清楚完全自主TL在RL环境中是否现实，或者是否真的有用。在大多数情况下，一个人会在循环的某个地方，完全的自主是没有必要的。相反，可以通过**学习映射**来补充人类对合适的映射的直觉，或者可以提出一组学习后的映射，然后由人类选择一个。当需要完全自主的迁移时，定义真实的场景是值得的，或者指定(有限的)人类交互将如何与映射学习方法耦合。

最后，我们希望**任务不变知识的概念能够得到扩展**。Agent空间和RRL技术不是在任务之间学习合适的表示，而是试图发现关于agent和agent动作的知识，这些知识可以在新的任务中直接重用。更好的技术能够成功地划分知识，将有用的迁移和无用的迁移分开，就更容易实现成功的转移，而不必消除不相关的偏见。

## 9.2 负迁移
在文献中，TL的大部分工作都集中在表明一种特定的转移方法是可信的。据我们所知，没有一个有一个定义良好的方法来根据一个或多个度量来确定方法何时会失败。虽然我们可以说通过迁移可以更快地改进目标任务中的学习，但目前我们还不能确定任意一对任务是否适合给定的转移方法。因此，迁移可能会产生错误的偏置，导致负迁移。

类似于Master的方法，可以通过模型预测误差测量任务相似，或区域迁移，考察了局部的任务相似性，而不是在每个任务级别，可以协助在决定agent是否应该迁移或agnet应该转移什么。然而，这两种方法都没有为其有效性提供任何理论保证。

作为为任务相似度定义度量为何困难的一个例子，请考虑图11中所示的一对任务，它们非常相似，但是直接传输策略或动作值函数将是有害的。图11(顶部)中的源任务是确定性的和离散的。代理从状态I开始，有一个操作可用：East。“走廊”中的其他状态两个适用的行动：East和West，但状态A除外，它也有North和South的动作。一旦agent在状态A中执行North或South，它将分别保持在状态B或C中，并继续进行自转换。除了状态B中的自我转换外，没有任何转换是有回报的。

![4ff44979b65b88581ce21057a83add90.png](:/186bb86d9e0541ad8355ad8264c1e886)

现在考虑图11(底部)中的目标任务，它与源任务相同，只是从$C'$的自转换是MDP中惟一得到奖励的转换。$Q^{*}(I'，East)$在目标任务(最优动作值函数，评价在状态I′)和在源任务中的$Q^{*}(I，East)$是一样的。的确，目标任务中的最优策略只在单个状态a '处不同，而最优动作值函数只在状态a '、B '和C '处不同。

避免负迁移的一种潜在方法是利用**互模拟(*bisimulation*)** 的思想。Ferns等人指出：
* 在MDPs的上下文中，当对于每个动作，它们获得相同的即时回报，并且具有相同的转换为等价状态类的概率时，互模拟可以粗略地描述为MDP状态空间上最大的等价关系，该关系精确地将两个状态联系起来。这意味着互相似状态导致本质上相同的长期行为。

然而，互模拟可能过于严格，因为状态要么相等要么不相等，而且在实际计算中可能很慢。Ferns等人(2005，2006)的工作将互模拟的概念放宽到可以更快地计算的(伪)度量，并给出了相似度度量，而不是布尔值。互模拟可以用来发现状态空间的区域，这些区域可以从一个任务转移到另一个任务，或者确定两个任务有多相似，尽管还没有显示出来。除此之外，或者可能正因为如此，目前还没有自动构造给定目标任务的源任务的方法。

**同态(*Homomorphisms*)** 是一种不同的抽象，它可以基于转换和奖励动态来定义MDPs之间的转换，在思想上类似于任务间映射，并已成功地用于迁移(Soni和Singh， 2006)。然而，发现同态是NP-hard(Ravindran and Barto， 2003a)，而同态通常是通过预测提供给学习者的。虽然这两种理论框架可能有助于避免负迁移，或确定两个任务何时是“迁移兼容的”，但需要做大量工作来确定这种方法在实践中是否可行，特别是如果代理是完全自主的(即没有被人类提供整个领域的知识)并且没有提供MDP的完整模型。

## 9.3 新方向
如上所述，RL领域的TL是机器学习的一个领域，在这个领域，实验工作已经超过了理论。而分类任务之间的转移理论已有一些研究(cf.， Baxter， 2000；这样的分析并不直接适用于RL设置。据我们所知，在RL 中，只有一篇分析转移理论性质的文章(Phillips， 2006)，其中作者使用了Kantorovich和两个MDPs的完整模型来计算一个任务中的最优策略在第二个任务中的执行情况。不幸的是，这种策略性能的计算可能比直接在目标任务中学习需要更多的计算。在RL中有相当大的空间和需要在于更多的理论工作上(cf.， Bowling and Veloso， 1999)。例如：
* 1. 提供关于特定源任务是否可以改进目标任务中的学习(给定特定类型的知识迁移)的保证。
* 2. 将知识迁移的数量(例如样本的数量)与源任务的改进相关联。
* 3. 定义什么是最优的任务间映射，并演示使用的任务间映射如何影响迁移效率。

本节的其余部分建议其他开放区域。

RL中的**概念漂移**(*Concept drift* )在本调查中没有任何工作直接涉及。概念漂移的概念与非平稳环境有关：在特定的时间点，环境可能会任意变化。正如Ramon等人所指出的，“对于迁移学习，当上下文发生变化时，它通常是已知的。对于概念漂移，这种变化通常是未经宣布的。“目前的在线学习方法可以通过不断学习来应对这些变化。然而，RL方法很可能是专门为收敛到一个策略而开发的，然后在概念更改时重新开始学习，从而获得更高的性能，无论这种漂移是公开的还是未公开的。

本调查中没有直接涉及的另一个问题是，**如何确定源任务训练的最优数量，以最小化目标任务训练时间或总训练时间**。如果源任务和目标任务相同，那么减少目标任务培训时间的目标将是微不足道的(通过最大化源任务培训时间)，而最小化总培训时间的目标将是不可能实现的。另一方面，如果源任务和目标任务是不相关的，则不可能通过迁移 来减少目标任务的训练时间，完全不进行源任务的训练将使总训练时间最小化。确定源任务训练时间的最优数量的计算或启发式方法很可能必须考虑这两个任务的结构、它们之间的关系以及使用的传输方法。这种优化在多步转移的情况下变得更加困难，因为有两个或多个任务可以训练不同的时间。

本次调查中的迁移方法采用了多种形式的源任务知识来更好地学习目标任务。但是，没有一个明确地说明这**两个任务之间的缩放差异**。例如，如果源任务以米为单位测量距离，而目标任务以英寸为单位测量距离，则必须手动更新常量，而不是学习。

另一个未解决的问题是，**如果agent的明确目的是加速目标任务中的学习，那么如何最好地探索源任务**。可以想象，与标准策略相比，非标准的学习或探索策略可能会产生更好的迁移结果。例如，探索更多的源任务的状态空间可能比只学习部分状态空间的准确的操作值函数更好。虽然目前还没有TL算法采用这种方法，但在非迁移环境下(S ims ek and Barto， 2006)，已经有一些关于学习策略的研究(没有试图最大化学习过程中积累的在线奖励)。

类似地，**一个知道自己的信息将用于目标任务的代理可能会决定记录信息，以便在源任务的训练过程中进行中途迁移**，而不是总是在源任务的学习结束时迁移信息。例如，Taylor等人(2007b)表明，在源任务中使用训练时间较短的策略比使用训练时间较长的策略更有效。虽然其他人也观察到类似的行为，但大多数工作表明源任务性能的提高与目标任务性能的提高相关。了解这种影响是如何发生的以及为什么会发生，将有助于确定将信息从一个任务转移到另一个任务的最合适时间。现在，我们提出了四种可能性，将当前的RL转移工作扩展到不同的学习环境，在这些学习环境中，迁移还没有被成功地应用。

* 首先，尽管有两篇论文(Banerjee and Stone， 2007；(Kuhlmann and Stone， 2007)在这项调查中考察了广泛的博弈，没有人考虑**重复范式博弈或随机博弈**(Shapley， 1953)。例如，一个人可以考虑学习如何与一组对手比赛，这样当一个新的对手被引入时，学习者可以快速地适应之前的策略之一，而不是完全重新学习策略。另一种选择是让代理学习如何玩一个游戏，然后将知识转移到另一个随机游戏中。由于RL和这两种游戏设置的相似之处，本次调查中描述的迁移方法可能只需要相对较少的修改就可以使用。

* 扩展传输的第二种可能性是进入**部分可观察MDPs** (POMDPs)领域。可以学习源POMDP，然后使用获得的知识启发式地加快目标POMDP中的规划。此外，因为它通常假定POMDP规划者给出关于一个任务的完整的和准确的模型，可以在学习之前分析比较源和目标任务，这是为了确定转移是否是有益的，如果是这样，如何最好地利用过去的知识。

* 第三，**多智能体MDP和POMDP学习者也可能能够成功地利用迁移**。本文调查的所有工作都没有关注显式多智能体学习(即，在联合行动空间中学习，或在(自适应的)对抗环境中学习，如Stone和Veloso 2000)，但现有的方法可能会扩展到协作多智能体环境。例如，当将问题表示为MMDP或DEC-MDP时，agent必须在联合动作空间上进行推理，或者明确地推理它们的动作如何影响其他动作。agent可能首先学习一组动作，然后随着时间的推移逐渐添加操作(或联合操作)，类似于在具有不同动作集的任务之间进行迁移。在分布式POMDPs中，对这种加速的需求尤为关键，因为已经证明NEXP-Complete可以最优地解决这些问题(Bernstein等，2002)。迁移是使这些问题更容易处理的一种可能的方法，但据我们所知，还没有提出这样的方法。

* 第四，如3.3节所述，RL中的MTL方法考虑从相同分布中按顺序绘制的任务序列。然而，在监督学习中，多任务学习通常包括同时学习多个任务。在某些上下文中，代理必须同时学习多个任务，例如在分层RL中，或者当代理具有多个奖励函数或目标时。完全指定这样一个场景，并扩展MTL方法来包含这个设置，可以为RL研究人员带来额外的工具，并帮助RL中的TL更接近在分类中的TL。

最后，**为了更好地评估TL方法，最好有一组标准的域和度量**。理想情况下，迁移学习应该有一个独立于领域的度量标准，但是这种度量标准是否存在还不清楚(参见第2节)。此外，目前还不清楚最优迁移意味着什么，但可能取决于所考虑的情景。分类和回归长期以来一直受益于标准度量标准，如精确度和召回率，一旦标准度量标准达成一致，转移方面的进展可能也会得到同样的增强。

标准测试集，如加州大学欧文分校的机器学习存储库(Asuncion and Newman， 2007)，也帮助了监督学习的增长和进步，但目前没有对应的RL存储库。此外，虽然分类中有一些用于转移学习的标准数据集，但RL中没有用于迁移学习的标准数据集。虽然在RL社区中有一些工作是标准化一个公共接口和一组基准测试任务，但是在RL社区中，还没有针对迁移学习提出这样的标准化。即使在没有这样一个框架的情况下，我们建议在这方面工作的作者必须：
* 明确指定设置：源任务学习时间忽视了吗？对源目标和目标任务之间的关系做了哪些假设?
* 使用多个度量来评估算法：没有一个度量能够从传输中获得所有可能的好处。
* 从经验或理论上比较新算法的性能：为了更好地评估新算法，应该使用单个任务的标准度量来比较现有算法

正如在2.1节中所讨论的，我们不认为TL对于RL方法的有效性是可以严格排序的，因为迁移的目标有很多。但是，通过标准化报告方法，TL算法可以更容易地进行比较，从而更容易在给定的实验设置中选择合适的方法。

我们希望在不久的将来能够解决TL问题，如本节中介绍的那些问题；我们的期望是，迁移学习将成为机器学习社区日益强大的工具。

## 参考文献：
Taylor M E, Stone P. Transfer Learning for Reinforcement Learning Domains: A Survey[J]. Journal of Machine Learning Research， 2009， 10(10):1633-1685.